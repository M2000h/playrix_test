{
 "cells": [
  {
   "cell_type": "code",
   "id": "53edbb40-aefd-422e-896e-3d76f3fd9eb7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install -U --no-cache-dir \"torch>=2.3\" \"huggingface_hub>=0.23.0\"\n",
    "!pip uninstall -y diffusers\n",
    "!pip install --no-cache-dir \"git+https://github.com/huggingface/diffusers.git@main\"\n",
    "!pip install --upgrade --no-cache-dir \"transformers>=4.44\" \"accelerate>=0.34\" safetensors peft bitsandbytes sentencepiece safetensors protobuf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34777733-6709-4b50-b199-a26e3ee2b042",
   "metadata": {},
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline, FluxTransformer2DModel, AutoencoderKL\n",
    "from transformers import CLIPTextModel, T5EncoderModel, AutoTokenizer\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "\n",
    "ckpt =  \"/workspace/models/flux-dev\"\n",
    "#ckpt =  \"/workspace/models/flux-kontext-dev\"\n",
    "\n",
    "clip = CLIPTextModel.from_pretrained(ckpt, subfolder=\"text_encoder\", torch_dtype=dtype)\n",
    "t5   = T5EncoderModel.from_pretrained(ckpt, subfolder=\"text_encoder_2\", torch_dtype=dtype)\n",
    "trf  = FluxTransformer2DModel.from_pretrained(ckpt, subfolder=\"transformer\", torch_dtype=dtype)\n",
    "vae  = AutoencoderKL.from_pretrained(ckpt, subfolder=\"vae\", torch_dtype=dtype)\n",
    "\n",
    "# Токенайзеры\n",
    "tok_clip = AutoTokenizer.from_pretrained(ckpt, subfolder=\"tokenizer\", use_fast=False)\n",
    "tok_t5   = AutoTokenizer.from_pretrained(ckpt, subfolder=\"tokenizer_2\", use_fast=True)\n",
    "\n",
    "# 2) Собираем пайплайн, подменяя компоненты\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    ckpt,\n",
    "    torch_dtype=dtype,\n",
    "    transformer=trf,\n",
    "    vae=vae,\n",
    "    text_encoder=clip,\n",
    "    text_encoder_2=t5,\n",
    "    tokenizer=tok_clip,\n",
    "    tokenizer_2=tok_t5,\n",
    ")\n",
    "pipe.to(\"cuda\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18f29eed-913d-41b9-8182-70c2658e0004",
   "metadata": {},
   "source": [
    "# Загружаем только стиль\n",
    "pipe.load_lora_weights(\"/workspace/weights/style/checkpoint-1000\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "88865cc1-47af-4ae1-a0e5-542503adaf5e",
   "metadata": {},
   "source": [
    "# Загружаем стиль и объект\n",
    "pipe.load_lora_weights(\"/workspace/weights/style/checkpoint-1000\", adapter_name=\"style\")\n",
    "pipe.load_lora_weights(\"/workspace/weights/Kremlin/checkpoint-800\", adapter_name=\"kreml\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b17520a-89a3-40d8-a196-d1eea7382b61",
   "metadata": {},
   "source": [
    "# Подбираем баланс\n",
    "pipe.set_adapters([\"style\", \"kreml\"], adapter_weights=[0.9, 0.3])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83e1c4ed-d93b-460d-a63b-a9724e863a91",
   "metadata": {},
   "source": [
    "# В случае image-to-image лучше ставить меньше 1\n",
    "pipe.set_adapters([\"default_0\"], adapter_weights=[0.5])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42abf1ea-46ee-4fa1-a7f3-8d44ba08ba08",
   "metadata": {},
   "source": [
    "prompt = \"Modern district with tall residential towers, wide sidewalks, cars, two people sleek high-rises, evening lights, cinematic, in <sksartist> style\"\n",
    "for i in range(90, 100):\n",
    "    image = pipe(prompt, num_inference_steps=36, guidance_scale=3.5, width=1024, height=1024).images[0]\n",
    "    image.save(f\"images/moscow_last_{i}.png\")\n",
    "    print(f\"images/moscow_last_{i}.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7ac1f80a-1f3b-4a36-af87-09692d3b4e8d",
   "metadata": {},
   "source": [
    "from diffusers.utils import load_image\n",
    "pipe.set_adapters([\"default_0\"], adapter_weights=[1])\n",
    "img = load_image(\"./dataset/MSU/2.jpg\").convert(\"RGB\")\n",
    "out = pipe(\n",
    "    image=img,                              # здесь image уже допустим\n",
    "    prompt=\"in <sksartist> style, dramatic bright lighting\",\n",
    "    guidance_scale=2.5,\n",
    "    num_inference_steps=36,\n",
    "    width=1024, height=1024,\n",
    "    generator=torch.Generator().manual_seed(234),\n",
    ").images[0]\n",
    "out.save(\"images/msu_from_orig2_new_dramatic1.png\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
